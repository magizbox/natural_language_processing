<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Vu Anh">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Word Vectors - Natural Language Processing</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../components/magiz-c-book/src/book.css" rel="stylesheet">
        <link href="../components/magiz-c-course/src/course.css" rel="stylesheet">
        <link href="../components/magiz-c-video/src/video.css" rel="stylesheet">
        <link href="../components/magiz-c-benchmark/src/benchmark.css" rel="stylesheet">
        <link href="../components/magiz-c-paper/src/paper.css" rel="stylesheet">
        <link href="../components/magiz-c-feed/src/feed.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->
	
	<script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-68978617-1', 'magizbox.com');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="..">Natural Language Processing</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="..">Home</a>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Docs <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../task/">Tasks</a>
</li>
                            
<li >
    <a href="../spelling_correction/">Spelling Correction</a>
</li>
                            
<li class="active">
    <a href="./">Word Vectors</a>
</li>
                            
<li >
    <a href="../ner_crf/">CRF in NER</a>
</li>
                            
<li >
    <a href="../entity_linking/">Entity Linking</a>
</li>
                            
<li >
    <a href="../application/">Applications</a>
</li>
                            
  <li class="dropdown-submenu">
    <a href="#">Languages</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../vietnlp/">Vietnamese</a>
</li>
            
<li >
    <a href="../englishnlp/">English</a>
</li>
    </ul>
  </li>
                        </ul>
                    </li>
                    <li >
                        <a href="../feed/"><span class='fa fa-rss'></span> News</a>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../spelling_correction/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../ner_crf/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#discrete-representation">Discrete Representation</a></li>
        <li class="main "><a href="#word2vec">Word2Vec</a></li>
        <li class="main "><a href="#glove">GloVe</a></li>
        <li class="main "><a href="#pre-trained-model">Pre-trained Model</a></li>
        <li class="main "><a href="#word-analogies">Word Analogies</a></li>
        <li class="main "><a href="#suggested-readings">Suggested Readings</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h2 id="discrete-representation">Discrete Representation</h2>
<p>Use a taxonomy like WordNet that has hypernyms (is-a) relationships</p>
<pre><code class="python">from nltk.corpus import wordnet as wn
panda = wn.synset(&quot;panda.n.01&quot;)
hyper = lambda s: s.hypernyms()
list(panda.closure(hyper))
</code></pre>

<pre><code>[Synset('procyonid.n.01'),
 Synset('carnivore.n.01'),
 Synset('placental.n.01'),
 Synset('mammal.n.01'),
 Synset('vertebrate.n.01'),
 Synset('chordate.n.01'),
 Synset('animal.n.01'),
 Synset('organism.n.01'),
 Synset('living_thing.n.01'),
 Synset('whole.n.02'),
 Synset('object.n.01'),
 Synset('physical_entity.n.01'),
 Synset('entity.n.01')]
</code></pre>

<pre><code class="python">wn.synsets(&quot;good&quot;)
</code></pre>

<pre><code>[Synset('good.n.01'),
 Synset('good.n.02'),
 Synset('good.n.03'),
 Synset('commodity.n.01'),
 Synset('good.a.01'),
 Synset('full.s.06'),
 Synset('good.a.03'),
 Synset('estimable.s.02'),
 Synset('beneficial.s.01'),
 ...
</code></pre>

<pre><code class="python">wn.synsets(&quot;bad&quot;)
</code></pre>

<pre><code>[Synset('bad.n.01'),
 Synset('bad.a.01'),
 Synset('bad.s.02'),
 Synset('bad.s.03'),
 Synset('bad.s.04'),
 Synset('regretful.a.01'),
 Synset('bad.s.06'),
 Synset('bad.s.07'),
 Synset('bad.s.08'),
 ...
</code></pre>

<p><strong>Problems with this discrete representation</strong></p>
<ul>
<li>Great as resource but missing nuances, e.g. <strong>synonyms</strong>: adept, expert, good, practiced, proficient, skillful?</li>
<li>Missing new words (impossible to keep up to date): wicked, badass, nifty, crack, ace, wizard, genius, ninjia</li>
<li>Subjective</li>
<li>Requires human labor to create and adapt</li>
<li>Hard to compute accurate word similarity</li>
</ul>
<h2 id="word2vec">Word2Vec</h2>
<p>Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.</p>
<p>Word2vec was created by a team of researchers led by Tomas Mikolov at Google. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms like Latent Semantic Analysis.</p>
<p><strong>Main Idea of Word2Vec</strong></p>
<ul>
<li>Instead of capturing cooccurrence counts directly,,</li>
<li>Predict surrounding words of every word</li>
<li>Both are quite similar, see "Glove: Global Vectors for Word Representation" by Pennington et at. (2014) and Levy and Goldberg (2014)... more later.</li>
<li>Faster and can easily incorporate a new sentence/document or add a word to the vocabulary.</li>
</ul>
<p><strong>Detail of Word2Vec</strong></p>
<ul>
<li>Predict surrounding words in a window of length m of every word.</li>
<li>Objective function: Maximize the log probability of any context word given the current cenetr word:</li>
</ul>
<p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{T} \sum_{t=1}^{T} \sum_{-m \le j \le m, j \ne 0} log\ p(w_{t+j}|w_t) </script>
</p>
<p>where <script type="math/tex"> \theta </script> represents all variables we optimize</p>
<ul>
<li>Predict surrounding words in a window of length m of every word</li>
<li>For <script type="math/tex"> p(w_{t+j}|w_t) </script> the simplest first formulation is</li>
</ul>
<p>
<script type="math/tex; mode=display">p(o|c) = \frac{exp(u^T_o v_c)}{\sum_{w=1}^{W} exp(u^T_w v_c)}</script>
</p>
<p>where o is the outside (or output) word id, c is the center word id, u and v are "center" and "outside" vectors of o and c</p>
<ul>
<li>Every word has two vectors!</li>
<li>This is essentially "dynamic" logistic regression</li>
</ul>
<p><strong>Linear Relationships in word2vec</strong></p>
<p>These representations are <em>very good</em> at encoding dimensions of similarity!</p>
<ul>
<li>Analogies testing dimensions of similarity can be solved quite well just by doing vector subtraction in the embedding space</li>
</ul>
<p>Syntactically</p>
<ul>
<li>
<script type="math/tex"> x_{apple} - x_{apples} \approx x_{car} - x_{cars} \approx x_{family} - x_{families} </script>
</li>
<li>Similarly for verb and adjective morphological forms</li>
</ul>
<p>Semantically (Semeval 2012 task 2)</p>
<ul>
<li>
<script type="math/tex"> x_{shirt} - x_{clothing} \approx x_{chair} - x_{furniture} </script>
</li>
<li>
<script type="math/tex"> x_{king} - x_{man} \approx x_{queen} - x_{woman} </script>
</li>
</ul>
<h2 id="glove">GloVe</h2>
<p><a href="https://nlp.stanford.edu/projects/glove/">Project</a>
- <a href="https://nlp.stanford.edu/projects/glove/">Highlights</a>
- <a href="https://nlp.stanford.edu/projects/glove/">Training</a>
- <a href="https://nlp.stanford.edu/projects/glove/">Model Overview</a></p>
<p>GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</p>
<h2 id="pre-trained-model">Pre-trained Model</h2>
<p><strong><a href="https://github.com/facebookresearch/fastText/">fastText</a></strong></p>
<p>Pre-trained word vectors for <code>294 languages</code>, trained on <em>Wikipedia</em> using fastText. These vectors in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.</p>
<p><strong><a href="https://nlp.stanford.edu/projects/glove/">glove</a></strong></p>
<p>Pre-trained word vectors. This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/.</p>
<p>Language: <code>English</code></p>
<ul>
<li><em>Wikipedia 2014</em> + <em>Gigaword 5</em> (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, &amp; 300d vectors, 822 MB download): <a href="http://nlp.stanford.edu/data/glove.6B.zip">glove.6B.zip</a></li>
<li><em>Common Crawl</em> (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): <a href="http://nlp.stanford.edu/data/glove.42B.300d.zip">glove.42B.300d.zip</a></li>
<li><em>Common Crawl</em> (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): <a href="http://nlp.stanford.edu/data/glove.840B.300d.zip">glove.840B.300d.zip</a></li>
<li><em>Twitter</em> (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, &amp; 200d vectors, 1.42 GB download): <a href="http://nlp.stanford.edu/data/glove.twitter.27B.zip">glove.twitter.27B.zip</a></li>
</ul>
<p><strong><a href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors">word2vec-GoogleNews-vectors</a></strong></p>
<p>Language: <code>English</code></p>
<p>Pre-trained <em>Google News</em> corpus (3 billion running words) word vector model (3 million 300-dimension English word vectors).</p>
<h2 id="word-analogies">Word Analogies</h2>
<p>Test for linear relationships, examined by Mikolov et at. (2014)</p>
<h2 id="suggested-readings">Suggested Readings</h2>
<ul>
<li>Simple Word Vector representations: word2vec, GloVe. <a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf">cs224d.stanford.edu</a>. Last Accessed: 2017-02-01.</li>
<li>FastText and Gensim word embeddings. <a href="https://rare-technologies.com/fasttext-and-gensim-word-embeddings/">rare-technologies.com</a>. Last Accessed: 2016-08-31.</li>
<li>Distributed Representations of Words and Phrases
and their Compositionality. <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">papers.nips.cc</a>. Last Accessed: 2013-12-05.</li>
<li>Efficient Estimation of Word Representations in Vector Space. <a href="https://arxiv.org/pdf/1301.3781.pdf">arxiv.org</a>. Last Accessed: 2013-01-16</li>
</ul></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '..';</script>
        <script data-main="../mkdocs/js/search.js" src="../mkdocs/js/require.js"></script>
        <script src="../js/base.js"></script>
        <script src="../components/magiz-c-book/src/gspreadsheet.js"></script>
        <script src="../components/underscore/underscore.js"></script>
        <script src="../components/magiz-c-paper/src/paper.js"></script>
        <script src="../components/magiz-c-course/src/course.js"></script>
        <script src="../components/magiz-c-benchmark/src/benchmark.js"></script>
        <script src="../components/magiz-c-benchmark/src/bootstrap-popup.js"></script>
        <script src="../components/magiz-c-book/src/book.js"></script>
        <script src="../components/magiz-c-video/src/video.js"></script>
        <script src="../components/magiz-c-feed/src/feed.js"></script>
        <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
